var documenterSearchIndex = {"docs":
[{"location":"manual/optimal_trajectories/#Choosing-Optimal-Numbers-of-Trajectories","page":"Choosing Optimal Numbers of Trajectories","title":"Choosing Optimal Numbers of Trajectories","text":"","category":"section"},{"location":"manual/optimal_trajectories/","page":"Choosing Optimal Numbers of Trajectories","title":"Choosing Optimal Numbers of Trajectories","text":"There is a balance between two things for choosing the number of trajectories:","category":"page"},{"location":"manual/optimal_trajectories/","page":"Choosing Optimal Numbers of Trajectories","title":"Choosing Optimal Numbers of Trajectories","text":"The number of trajectories needs to be high enough that the work per kernel is sufficient to overcome the kernel call cost.\nMore trajectories means that every trajectory will need more time steps, since the adaptivity syncs all solves.","category":"page"},{"location":"manual/optimal_trajectories/","page":"Choosing Optimal Numbers of Trajectories","title":"Choosing Optimal Numbers of Trajectories","text":"From our testing, the balance is found at around 10,000 trajectories being optimal for EnsembleGPUArray, since it has higher kernel call costs because every internal operation of the ODE solver requires a kernel call. Thus, for larger sets of trajectories, use a batch size of 10,000. Of course, benchmark for yourself on your own setup, as all GPUs are different.","category":"page"},{"location":"manual/optimal_trajectories/","page":"Choosing Optimal Numbers of Trajectories","title":"Choosing Optimal Numbers of Trajectories","text":"On the other hand, EnsembleGPUKernel fuses the entire GPU solve into a single kernel, greatly reducing the kernel call cost. This means longer or more expensive ODE solves will require even less of a percentage of time kernel launching, making the cutoff much smaller. We see some cases with around 100 ODEs being viable with EnsembleGPUKernel. Again, this is highly dependent on the ODE and the chosen GPU and thus one will need to benchmark to get accurate numbers for their system, this is merely a ballpark estimate.","category":"page"},{"location":"tutorials/multigpu/#multigpu","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"","category":"section"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"note: Note\nThis tutorial assumes one already has familiarity with EnsembleGPUArray and EnsembleGPUKernel. Please see the Lorenz equation tutorial before reading this tutorial!","category":"page"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"In this tutorial, we will show how to increase the number of trajectories that can be computed in parallel by setting up and using a multi-GPU solve. For this, we will set up one Julia process for each GPU and let the internal pmap system of EnsembleGPUArray parallelize the system across all of our GPUs. Let's dig in.","category":"page"},{"location":"tutorials/multigpu/#Setting-Up-a-Multi-GPU-Julia-Environment","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up a Multi-GPU Julia Environment","text":"","category":"section"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"To set up a multi-GPU environment, first set up processes such that each process has a different GPU. For example:","category":"page"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"# Setup processes with different CUDA devices\nusing Distributed\nnumgpus = 1\naddprocs(numgpus)\nimport CUDA\n\nlet gpuworkers = asyncmap(collect(zip(workers(), CUDA.devices()))) do (p, d)\n        remotecall_wait(CUDA.device!, p, d)\n        p\n    end\nend","category":"page"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"Then set up the calls to work with distributed processes:","category":"page"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"@everywhere using DiffEqGPU, CUDA, OrdinaryDiffEq, Test, Random\n\n@everywhere begin\n    function lorenz_distributed(du, u, p, t)\n        du[1] = p[1] * (u[2] - u[1])\n        du[2] = u[1] * (p[2] - u[3]) - u[2]\n        du[3] = u[1] * u[2] - p[3] * u[3]\n    end\n    CUDA.allowscalar(false)\n    u0 = Float32[1.0; 0.0; 0.0]\n    tspan = (0.0f0, 100.0f0)\n    p = [10.0f0, 28.0f0, 8 / 3.0f0]\n    Random.seed!(1)\n    function prob_func_distributed(prob, i, repeat)\n        remake(prob, p = rand(3) .* p)\n    end\nend","category":"page"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"Now each batch will run on separate GPUs. Thus, we need to use the batch_size keyword argument from the Ensemble interface to ensure there are multiple batches. Let's solve 40,000 trajectories, batching 10,000 trajectories at a time:","category":"page"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"prob = ODEProblem(lorenz_distributed, u0, tspan, p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func_distributed)\n\n@time sol2 = solve(monteprob, Tsit5(), EnsembleGPUArray(CUDA.CUDABackend()),\n    trajectories = 40_000,\n    batch_size = 10_000, saveat = 1.0f0)","category":"page"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"This will pmap over the batches, and thus if you have 4 processes each with a GPU, each batch of 10,000 trajectories will be run simultaneously. If you have two processes with two GPUs, this will do two sets of 10,000 at a time.","category":"page"},{"location":"tutorials/multigpu/#Example-Multi-GPU-Script","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Example Multi-GPU Script","text":"","category":"section"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"In this example, we know we have a 2-GPU system (1 eGPU), and we split the work across the two by directly defining the devices on the two worker processes:","category":"page"},{"location":"tutorials/multigpu/","page":"Setting Up Multi-GPU Parallel Parameter Sweeps","title":"Setting Up Multi-GPU Parallel Parameter Sweeps","text":"using DiffEqGPU, CUDA, OrdinaryDiffEq, Test\nCUDA.device!(0)\n\nusing Distributed\naddprocs(2)\n@everywhere using DiffEqGPU, CUDA, OrdinaryDiffEq, Test, Random\n\n@everywhere begin\n    function lorenz_distributed(du, u, p, t)\n        du[1] = p[1] * (u[2] - u[1])\n        du[2] = u[1] * (p[2] - u[3]) - u[2]\n        du[3] = u[1] * u[2] - p[3] * u[3]\n    end\n    CUDA.allowscalar(false)\n    u0 = Float32[1.0; 0.0; 0.0]\n    tspan = (0.0f0, 100.0f0)\n    p = [10.0f0, 28.0f0, 8 / 3.0f0]\n    Random.seed!(1)\n    pre_p_distributed = [rand(Float32, 3) for i in 1:100_000]\n    function prob_func_distributed(prob, i, repeat)\n        remake(prob, p = pre_p_distributed[i] .* p)\n    end\nend\n\n@sync begin\n    @spawnat 2 begin\n        CUDA.allowscalar(false)\n        CUDA.device!(0)\n    end\n    @spawnat 3 begin\n        CUDA.allowscalar(false)\n        CUDA.device!(1)\n    end\nend\n\nCUDA.allowscalar(false)\nprob = ODEProblem(lorenz_distributed, u0, tspan, p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func_distributed)\n\n@time sol = solve(monteprob, Tsit5(), EnsembleGPUArray(CUDA.CUDABackend()),\n    trajectories = 100_000,\n    batch_size = 50_000, saveat = 1.0f0)","category":"page"},{"location":"tutorials/parallel_callbacks/#events","page":"Massively Parallel ODE Solving with Event Handling and Callbacks","title":"Massively Parallel ODE Solving with Event Handling and Callbacks","text":"","category":"section"},{"location":"tutorials/parallel_callbacks/","page":"Massively Parallel ODE Solving with Event Handling and Callbacks","title":"Massively Parallel ODE Solving with Event Handling and Callbacks","text":"using DiffEqGPU, StaticArrays, OrdinaryDiffEq, CUDA\nfunction f(u, p, t)\n    du1 = -u[1]\n    return SVector{1}(du1)\nend\n\nu0 = @SVector [10.0f0]\nprob = ODEProblem{false}(f, u0, (0.0f0, 10.0f0))\nprob_func = (prob, i, repeat) -> remake(prob, p = prob.p)\nmonteprob = EnsembleProblem(prob, safetycopy = false)\n\ncondition(u, t, integrator) = t == 4.0f0\naffect!(integrator) = integrator.u += @SVector[10.0f0]\n\ngpu_cb = DiscreteCallback(condition, affect!; save_positions = (false, false))\n\nsol = solve(monteprob, GPUTsit5(), EnsembleGPUKernel(CUDA.CUDABackend()),\n    trajectories = 10,\n    adaptive = false, dt = 0.01f0, callback = gpu_cb, merge_callbacks = true,\n    tstops = [4.0f0])","category":"page"},{"location":"manual/backends/#Compute-Backends-(GPU-Choices)","page":"Compute Backends (GPU Choices)","title":"Compute Backends (GPU Choices)","text":"","category":"section"},{"location":"manual/backends/","page":"Compute Backends (GPU Choices)","title":"Compute Backends (GPU Choices)","text":"DiffEqGPU.jl supports a multitude of different GPU devices. These must be chosen during the construction of the EnsembleGPUArray and EnsembleGPUKernel construction and correpond to the compute backends of KernelAbstractions.jl. The choices for backends are:","category":"page"},{"location":"manual/backends/","page":"Compute Backends (GPU Choices)","title":"Compute Backends (GPU Choices)","text":"CUDA.CUDABackend(): For NVIDIA GPUs via code generation for CUDA kernels.\nAMDGPU.ROCBackend(): For AMD GPUs via code generation for ROCm kernels.\noneAPI.oneAPIBackend(): For Intel GPUs via code generation for OneAPI kernels.\nMetal.MetalBackend(): For Apple Silicon (M-Series such as M1 or M2) via code generation for Metal kernels.","category":"page"},{"location":"manual/backends/","page":"Compute Backends (GPU Choices)","title":"Compute Backends (GPU Choices)","text":"This is used for example like EnsembleGPUKernel(oneAPI.oneAPIBackend()) to enable the computations for Intel GPUs. The choice of backend is mandatory and requires the installation of the respective package. Thus for example, using the OneAPI backend requires that the user has successfully installed oneAPI.jl and has an Intel GPU.","category":"page"},{"location":"manual/ensemblegpuarray/#EnsembleGPUArray","page":"EnsembleGPUArray","title":"EnsembleGPUArray","text":"","category":"section"},{"location":"manual/ensemblegpuarray/#API","page":"EnsembleGPUArray","title":"API","text":"","category":"section"},{"location":"manual/ensemblegpuarray/","page":"EnsembleGPUArray","title":"EnsembleGPUArray","text":"EnsembleGPUArray\nEnsembleCPUArray","category":"page"},{"location":"manual/ensemblegpuarray/#DiffEqGPU.EnsembleGPUArray","page":"EnsembleGPUArray","title":"DiffEqGPU.EnsembleGPUArray","text":"EnsembleGPUArray(backend,cpu_offload = 0.2)\n\nAn EnsembleArrayAlgorithm which utilizes the GPU kernels to parallelize each ODE solve with their separate ODE integrator on each kernel.\n\nPositional Arguments\n\nbackend: the KernelAbstractions backend for performing the computation.\ncpu_offload: the percentage of trajectories to offload to the CPU. Default is 0.2 or 20% of trajectories.\n\nLimitations\n\nEnsembleGPUArray requires being able to generate a kernel for f using KernelAbstractons.jl and solving the resulting ODE defined over CuArray input types. This introduces the following limitations on its usage:\n\nNot all standard Julia f functions are allowed. Only Julia f functions which are capable of being compiled into a GPU kernel are allowed. This notably means that certain features of Julia can cause issues inside of kernel, like:\nAllocating memory (building arrays)\nLinear algebra (anything that calls BLAS)\nBroadcast\nNot all ODE solvers are allowed, only those from OrdinaryDiffEq.jl. The tested feature set from OrdinaryDiffEq.jl includes:\nExplicit Runge-Kutta methods\nImplicit Runge-Kutta methods\nRosenbrock methods\nDiscreteCallbacks and ContinuousCallbacks\nStiff ODEs require the analytical solution of every derivative function it requires. For example, Rosenbrock methods require the Jacobian and the gradient with respect to time, and so these two functions are required to be given. Note that they can be generated by the modelingtoolkitize approach.\nTo use multiple GPUs over clusters, one must manually set up one process per GPU. See the multi-GPU tutorial for more details.\n\nwarn: Warn\nCallbacks with terminate! does not work well with EnsembleGPUArray as the entire integration will hault when any of the trajectories hault. Use with caution.\n\nExample\n\nusing DiffEqGPU, CUDA, OrdinaryDiffEq\nfunction lorenz(du, u, p, t)\n    du[1] = p[1] * (u[2] - u[1])\n    du[2] = u[1] * (p[2] - u[3]) - u[2]\n    du[3] = u[1] * u[2] - p[3] * u[3]\nend\n\nu0 = Float32[1.0;0.0;0.0]\ntspan = (0.0f0,100.0f0)\np = [10.0f0,28.0f0,8/3f0]\nprob = ODEProblem(lorenz,u0,tspan,p)\nprob_func = (prob,i,repeat) -> remake(prob,p=rand(Float32,3).*p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func, safetycopy=false)\n@time sol = solve(monteprob,Tsit5(),EnsembleGPUArray(CUDADevice()),trajectories=10_000,saveat=1.0f0)\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpuarray/#DiffEqGPU.EnsembleCPUArray","page":"EnsembleGPUArray","title":"DiffEqGPU.EnsembleCPUArray","text":"EnsembleCPUArray(cpu_offload = 0.2)\n\nAn EnsembleArrayAlgorithm which utilizes the CPU kernels to parallelize each ODE solve with their separate ODE integrator on each kernel. This method is meant to be a debugging counterpart to EnsembleGPUArray, having the same behavior and using the same KernelAbstractions.jl process to build the combined ODE, but without the restrictions of f being a GPU-compatible kernel function.\n\nIt is unlikely that this method is useful beyond library development and debugging, as almost any case should be faster with EnsembleThreads or EnsembleDistributed.\n\n\n\n\n\n","category":"type"},{"location":"examples/ad/#Using-GPU-accelerated-Ensembles-with-Automatic-Differentiation","page":"Using GPU-accelerated Ensembles with Automatic Differentiation","title":"Using GPU-accelerated Ensembles with Automatic Differentiation","text":"","category":"section"},{"location":"examples/ad/","page":"Using GPU-accelerated Ensembles with Automatic Differentiation","title":"Using GPU-accelerated Ensembles with Automatic Differentiation","text":"EnsembleGPUArray comes with derivative overloads for reverse mode automatic differentiation, and thus can be thrown into deep learning training loops. The following is an example of this use:","category":"page"},{"location":"examples/ad/","page":"Using GPU-accelerated Ensembles with Automatic Differentiation","title":"Using GPU-accelerated Ensembles with Automatic Differentiation","text":"using OrdinaryDiffEq, SciMLSensitivity, Flux, DiffEqGPU, CUDA, Test\nCUDA.allowscalar(false)\n\nfunction modelf(du, u, p, t)\n    du[1] = 1.01 * u[1] * p[1] * p[2]\nend\n\nfunction model()\n    prob = ODEProblem(modelf, u0, (0.0, 1.0), pa)\n\n    function prob_func(prob, i, repeat)\n        remake(prob, u0 = 0.5 .+ i / 100 .* prob.u0)\n    end\n\n    ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n    solve(ensemble_prob, Tsit5(), EnsembleGPUArray(CUDA.CUDABackend()), saveat = 0.1,\n        trajectories = 10)\nend\n\n# loss function\nloss() = sum(abs2, 1.0 .- Array(model()))\n\ndata = Iterators.repeated((), 10)\n\ncb = function () # callback function to observe training\n    @show loss()\nend\n\npa = [1.0, 2.0]\nu0 = [3.0]\nopt = ADAM(0.1)\nprintln(\"Starting to train\")\n\nl1 = loss()\n\nFlux.@epochs 10 Flux.train!(loss, Flux.params([pa]), data, opt; cb = cb)","category":"page"},{"location":"examples/ad/","page":"Using GPU-accelerated Ensembles with Automatic Differentiation","title":"Using GPU-accelerated Ensembles with Automatic Differentiation","text":"Forward-mode automatic differentiation works as well, as demonstrated by its capability to recompile for Dual number arithmetic:","category":"page"},{"location":"examples/ad/","page":"Using GPU-accelerated Ensembles with Automatic Differentiation","title":"Using GPU-accelerated Ensembles with Automatic Differentiation","text":"using OrdinaryDiffEq, DiffEqGPU, ForwardDiff, Test\n\nfunction lorenz(du, u, p, t)\n    du[1] = p[1] * (u[2] - u[1])\n    du[2] = u[1] * (p[2] - u[3]) - u[2]\n    du[3] = u[1] * u[2] - p[3] * u[3]\nend\n\nu0 = [ForwardDiff.Dual(1.0f0, (1.0, 0.0, 0.0)), ForwardDiff.Dual(0.0f0, (0.0, 1.0, 0.0)),\n    ForwardDiff.Dual(0.0f0, (0.0, 0.0, 1.0))]\ntspan = (0.0f0, 100.0f0)\np = (10.0f0, 28.0f0, 8 / 3.0f0)\nprob = ODEProblem{true, SciMLBase.FullSpecialize}(lorenz, u0, tspan, p)\nprob_func = (prob, i, repeat) -> remake(prob, p = rand(Float32, 3) .* p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func)\n@time sol = solve(monteprob, Tsit5(), EnsembleGPUArray(CUDA.CUDABackend()),\n    trajectories = 10_000,\n    saveat = 1.0f0)","category":"page"},{"location":"tutorials/weak_order_conv_sde/#sdeweakconv","page":"Using the EnsembleGPUKernel SDE solvers for the expectation of SDEs ","title":"Using the EnsembleGPUKernel SDE solvers for the expectation of SDEs ","text":"","category":"section"},{"location":"tutorials/weak_order_conv_sde/","page":"Using the EnsembleGPUKernel SDE solvers for the expectation of SDEs ","title":"Using the EnsembleGPUKernel SDE solvers for the expectation of SDEs ","text":"Solving the SDEProblem using weak methods on multiple trajectories helps to generate the expectation of the stochastic process. With the lower overhead of EnsembleGPUKernel API, these calculations can be done in parallel on GPU, potentially being faster.","category":"page"},{"location":"tutorials/weak_order_conv_sde/","page":"Using the EnsembleGPUKernel SDE solvers for the expectation of SDEs ","title":"Using the EnsembleGPUKernel SDE solvers for the expectation of SDEs ","text":"The example below provides a way to calculate the expectation time-series of a linear SDE:","category":"page"},{"location":"tutorials/weak_order_conv_sde/","page":"Using the EnsembleGPUKernel SDE solvers for the expectation of SDEs ","title":"Using the EnsembleGPUKernel SDE solvers for the expectation of SDEs ","text":"using DiffEqGPU, OrdinaryDiffEq, StaticArrays, LinearAlgebra, Statistics\n\nnum_trajectories = 10_000\n\n# Defining the Problem\n# dX = pudt + qudW\nu₀ = SA[0.1f0]\nf(u, p, t) = SA[p[1] * u[1]]\ng(u, p, t) = SA[p[2] * u[1]]\ntspan = (0.0f0, 1.0f0)\np = SA[1.5f0, 0.01f0]\n\nprob = SDEProblem(f, g, u₀, tspan, p; seed = 1234)\n\nmonteprob = EnsembleProblem(prob)\n\nsol = solve(monteprob, GPUEM(), EnsembleGPUKernel(0.0), dt = Float32(1 // 2^8),\n    trajectories = num_trajectories, adaptive = false)\n\nsol_array = Array(sol)\n\nts = sol[1].t\n\nus_calc = reshape(mean(sol_array, dims = 3), size(sol_array, 2))\n\nus_expect = u₀ .* exp.(p[1] * ts)\n\nusing Plots\nplot(ts, us_expect, lw = 5,\n    xaxis = \"Time (t)\", yaxis = \"y(t)\", label = \"True Expected value\")\n\nplot!(ts, us_calc, lw = 3, ls = :dash, label = \"Caculated Expected value\")","category":"page"},{"location":"examples/reductions/#Batched-Reductions-for-Lowering-Peak-Memory-Requirements","page":"Batched Reductions for Lowering Peak Memory Requirements","title":"Batched Reductions for Lowering Peak Memory Requirements","text":"","category":"section"},{"location":"examples/reductions/","page":"Batched Reductions for Lowering Peak Memory Requirements","title":"Batched Reductions for Lowering Peak Memory Requirements","text":"Just as in the regular form of the DifferentialEquations.jl ensemble interface, a reduction function can be given to reduce between batches. Here we show an example of running 20 ODEs at a time, grabbing its value at the end, and reducing by summing all the values. This then allows for only saving the sum of the previous batches, boosting the trajectory count to an amount that is higher than would fit in memory, and only saving the summed values.","category":"page"},{"location":"examples/reductions/","page":"Batched Reductions for Lowering Peak Memory Requirements","title":"Batched Reductions for Lowering Peak Memory Requirements","text":"using OrdinaryDiffEq, DiffEqGPU, CUDA\n\nseed = 100\nusing Random;\nRandom.seed!(seed);\nra = rand(100)\n\nfunction f!(du, u, p, t)\n    du[1] = 1.01 * u[1]\nend\n\nprob = ODEProblem(f!, [0.5], (0.0, 1.0))\n\nfunction output_func(sol, i)\n    last(sol), false\nend\n\nfunction prob_func(prob, i, repeat)\n    remake(prob, u0 = ra[i] * prob.u0)\nend\n\nfunction reduction(u, batch, I)\n    u .+ sum(batch), false\nend\n\nprob2 = EnsembleProblem(prob, prob_func = prob_func, output_func = output_func,\n    reduction = reduction, u_init = Vector{eltype(prob.u0)}([0.0]))\nsim4 = solve(prob2, Tsit5(), EnsembleGPUArray(CUDA.CUDABackend()), trajectories = 100,\n    batch_size = 20)","category":"page"},{"location":"examples/sde/#GPU-Parallel-Solving-of-Stochastic-Differential-Equations","page":"GPU Parallel Solving of Stochastic Differential Equations","title":"GPU Parallel Solving of Stochastic Differential Equations","text":"","category":"section"},{"location":"examples/sde/","page":"GPU Parallel Solving of Stochastic Differential Equations","title":"GPU Parallel Solving of Stochastic Differential Equations","text":"One major application of DiffEqGPU is for computing ensemble statistics of SDE solutions using EnsembleGPUArray. The following demonstrates using this technique to generate large ensembles of solutions for a diagonal noise SDE with a high order adaptive method:","category":"page"},{"location":"examples/sde/","page":"GPU Parallel Solving of Stochastic Differential Equations","title":"GPU Parallel Solving of Stochastic Differential Equations","text":"using DiffEqGPU, CUDA, StochasticDiffEq\n\nfunction lorenz(du, u, p, t)\n    du[1] = p[1] * (u[2] - u[1])\n    du[2] = u[1] * (p[2] - u[3]) - u[2]\n    du[3] = u[1] * u[2] - p[3] * u[3]\nend\n\nfunction multiplicative_noise(du, u, p, t)\n    du[1] = 0.1 * u[1]\n    du[2] = 0.1 * u[2]\n    du[3] = 0.1 * u[3]\nend\n\nCUDA.allowscalar(false)\nu0 = Float32[1.0; 0.0; 0.0]\ntspan = (0.0f0, 10.0f0)\np = (10.0f0, 28.0f0, 8 / 3.0f0)\nprob = SDEProblem(lorenz, multiplicative_noise, u0, tspan, p)\nconst pre_p = [rand(Float32, 3) for i in 1:10_000]\nprob_func = (prob, i, repeat) -> remake(prob, p = pre_p[i] .* p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func)\nsol = solve(monteprob, SOSRI(), EnsembleGPUArray(CUDA.CUDABackend()), trajectories = 10_000,\n    saveat = 1.0f0)","category":"page"},{"location":"tutorials/gpu_ensemble_basic/#lorenz","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"","category":"section"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"For example, the following solves the Lorenz equation with 10,000 separate random parameters on the GPU. To start, we create a normal EnsembleProblem as per DifferentialEquations.jl. Here's a perfectly good multithreaded CPU parallelized Lorenz solve over randomized parameters:","category":"page"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"using DiffEqGPU, OrdinaryDiffEq, CUDA\nfunction lorenz(du, u, p, t)\n    du[1] = p[1] * (u[2] - u[1])\n    du[2] = u[1] * (p[2] - u[3]) - u[2]\n    du[3] = u[1] * u[2] - p[3] * u[3]\nend\n\nu0 = Float32[1.0; 0.0; 0.0]\ntspan = (0.0f0, 100.0f0)\np = [10.0f0, 28.0f0, 8 / 3.0f0]\nprob = ODEProblem(lorenz, u0, tspan, p)\nprob_func = (prob, i, repeat) -> remake(prob, p = rand(Float32, 3) .* p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func, safetycopy = false)\nsol = solve(monteprob, Tsit5(), EnsembleThreads(), trajectories = 10_000, saveat = 1.0f0);","category":"page"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"Changing this to being GPU-parallelized is as simple as changing the ensemble method to EnsembleGPUArray:","category":"page"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"sol = solve(monteprob, Tsit5(), EnsembleGPUArray(CUDA.CUDABackend()), trajectories = 10_000,\n    saveat = 1.0f0);","category":"page"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"and voilà, the method is re-compiled to parallelize the solves over a GPU!","category":"page"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"While EnsembleGPUArray has a bit of overhead due to its form of GPU code construction, EnsembleGPUKernel is a more restrictive GPU-itizing algorithm that achieves a much lower overhead in kernel launching costs. However, it requires this problem to be written in out-of-place form and use special solvers. This looks like:","category":"page"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"using DiffEqGPU, OrdinaryDiffEq, StaticArrays, CUDA\n\nfunction lorenz2(u, p, t)\n    σ = p[1]\n    ρ = p[2]\n    β = p[3]\n    du1 = σ * (u[2] - u[1])\n    du2 = u[1] * (ρ - u[3]) - u[2]\n    du3 = u[1] * u[2] - β * u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nu0 = @SVector [1.0f0; 0.0f0; 0.0f0]\ntspan = (0.0f0, 10.0f0)\np = @SVector [10.0f0, 28.0f0, 8 / 3.0f0]\nprob = ODEProblem{false}(lorenz2, u0, tspan, p)\nprob_func = (prob, i, repeat) -> remake(prob, p = (@SVector rand(Float32, 3)) .* p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func, safetycopy = false)\nsol = solve(monteprob, GPUTsit5(), EnsembleGPUKernel(CUDA.CUDABackend()),\n    trajectories = 10_000,\n    saveat = 1.0f0)","category":"page"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"Note that this form is also compatible with EnsembleThreads(), and EnsembleGPUArray(), so EnsembleGPUKernel() simply supports a subset of possible problem types. For more information on the limitations of EnsembleGPUKernel(), see its docstring.","category":"page"},{"location":"tutorials/gpu_ensemble_basic/#Using-Stiff-ODE-Solvers-with-EnsembleGPUArray","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Using Stiff ODE Solvers with EnsembleGPUArray","text":"","category":"section"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"DiffEqGPU also supports more advanced features than shown above. Other tutorials dive into handling events or callbacks and multi-GPU parallelism. But the simplest thing to show is that the generality of solvers allows for other types of equations. For example, one can handle stiff ODEs with EnsembleGPUArray simply by using a stiff ODE solver. Note that, as explained in the docstring, analytical derivatives (Jacobian and time gradient) must be supplied. For the Lorenz equation, this looks like:","category":"page"},{"location":"tutorials/gpu_ensemble_basic/","page":"Massively Data-Parallel ODE Solving the Lorenz Equation","title":"Massively Data-Parallel ODE Solving the Lorenz Equation","text":"function lorenz_jac(J, u, p, t)\n    σ = p[1]\n    ρ = p[2]\n    β = p[3]\n    x = u[1]\n    y = u[2]\n    z = u[3]\n    J[1, 1] = -σ\n    J[2, 1] = ρ - z\n    J[3, 1] = y\n    J[1, 2] = σ\n    J[2, 2] = -1\n    J[3, 2] = x\n    J[1, 3] = 0\n    J[2, 3] = -x\n    J[3, 3] = -β\nend\n\nfunction lorenz_tgrad(J, u, p, t)\n    nothing\nend\n\nu0 = Float32[1.0; 0.0; 0.0]\ntspan = (0.0f0, 100.0f0)\np = [10.0f0, 28.0f0, 8 / 3.0f0]\nfunc = ODEFunction(lorenz, jac = lorenz_jac, tgrad = lorenz_tgrad)\nprob_jac = ODEProblem(func, u0, tspan, p)\nmonteprob_jac = EnsembleProblem(prob_jac, prob_func = prob_func)\n\nsolve(monteprob_jac, Rodas5(), EnsembleGPUArray(CUDA.CUDABackend()), trajectories = 10_000,\n    saveat = 1.0f0)","category":"page"},{"location":"tutorials/lower_level_api/#lowerlevel","page":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","title":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","text":"","category":"section"},{"location":"tutorials/lower_level_api/","page":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","title":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","text":"EnsembleGPUKernel is designed to match the SciML ensemble interface in order to allow for directly converting CPU code to GPU code without any code changes. However, this hiding of the GPU aspects decreases the overall performance as it always transfers the problem to the GPU and the result back to the CPU for the user. These overheads can be removed by directly using the lower level API elements of EnsembleGPUKernel.","category":"page"},{"location":"tutorials/lower_level_api/","page":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","title":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","text":"The example below provides a way to generate solves using the lower level API with lower overheads:","category":"page"},{"location":"tutorials/lower_level_api/","page":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","title":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","text":"using DiffEqGPU, OrdinaryDiffEq, StaticArrays, CUDA, DiffEqBase\n\ntrajectories = 10_000\n\nfunction lorenz(u, p, t)\n    σ = p[1]\n    ρ = p[2]\n    β = p[3]\n    du1 = σ * (u[2] - u[1])\n    du2 = u[1] * (ρ - u[3]) - u[2]\n    du3 = u[1] * u[2] - β * u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nu0 = @SVector [1.0f0; 0.0f0; 0.0f0]\ntspan = (0.0f0, 10.0f0)\np = @SVector [10.0f0, 28.0f0, 8 / 3.0f0]\nprob = ODEProblem{false}(lorenz, u0, tspan, p)\n\n## Building different problems for different parameters\nprobs = map(1:trajectories) do i\n    remake(prob, p = (@SVector rand(Float32, 3)) .* p)\nend\n\n## Move the arrays to the GPU\nprobs = cu(probs)\n\n## Finally use the lower API for faster solves! (Fixed time-stepping)\n\n# Run once for compilation\n@time CUDA.@sync ts, us = DiffEqGPU.vectorized_solve(probs, prob, GPUTsit5();\n    save_everystep = false, dt = 0.1f0)\n\n@time CUDA.@sync ts, us = DiffEqGPU.vectorized_solve(probs, prob, GPUTsit5();\n    save_everystep = false, dt = 0.1f0)\n\n## Adaptive time-stepping\n# Run once for compilation\n@time CUDA.@sync ts, us = DiffEqGPU.vectorized_asolve(probs, prob, GPUTsit5();\n    save_everystep = false, dt = 0.1f0)\n\n@time CUDA.@sync ts, us = DiffEqGPU.vectorized_asolve(probs, prob, GPUTsit5();\n    save_everystep = false, dt = 0.1f0)","category":"page"},{"location":"tutorials/lower_level_api/","page":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","title":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","text":"Note that the core is the function DiffEqGPU.vectorized_solve which is the solver for the CUDA-based probs which uses the manually converted problems, and returns us which is a vector of CuArrays for the solution.","category":"page"},{"location":"tutorials/lower_level_api/","page":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","title":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","text":"Similarily, there exists a lower-level API for EnsembleGPUArray as well, primarily for benchmarking purposes. The solution returned for state (sol.u) is a matrix having columns as different parameter-parallel solutions for the ensemble problem. An example is shown below:","category":"page"},{"location":"tutorials/lower_level_api/","page":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","title":"Using the Lower Level API for Decreased Overhead with GPU acclerated Ensembles","text":"using DiffEqGPU, OrdinaryDiffEq, StaticArrays, CUDA, DiffEqBase\n\ntrajectories = 10_000\n\nfunction lorenz(u, p, t)\n    σ = p[1]\n    ρ = p[2]\n    β = p[3]\n    du1 = σ * (u[2] - u[1])\n    du2 = u[1] * (ρ - u[3]) - u[2]\n    du3 = u[1] * u[2] - β * u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nu0 = @SVector [1.0f0; 0.0f0; 0.0f0]\ntspan = (0.0f0, 10.0f0)\np = @SVector [10.0f0, 28.0f0, 8 / 3.0f0]\nprob = ODEProblem{false}(lorenz, u0, tspan, p)\n\n## Building different problems for different parameters\nbatch = 1:trajectories\nprobs = map(batch) do i\n    remake(prob, p = (@SVector rand(Float32, 3)) .* p)\nend\n\n## Finally use the lower API for faster solves! (Fixed time-stepping)\n\n@time CUDA.@sync sol = DiffEqGPU.vectorized_map_solve(probs, Tsit5(), EnsembleGPUArray(0.0),\n    batch, false, dt = 0.001f0,\n    save_everystep = false, dense = false)\n\n## Adaptive time-stepping (Notice the boolean argument)\n@time CUDA.@sync sol = DiffEqGPU.vectorized_map_solve(probs, Tsit5(), EnsembleGPUArray(0.0),\n    batch, true, dt = 0.001f0,\n    save_everystep = false, dense = false)","category":"page"},{"location":"tutorials/within_method_gpu/#withingpu","page":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","title":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","text":"","category":"section"},{"location":"tutorials/within_method_gpu/","page":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","title":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","text":"Within-Method GPU Parallelism for ODE solvers is a method for accelerating large ODE solves with regularity, i.e., only using array-based “vectorized” operations like linear algebra, maps, and broadcast statements. In these cases, the solve can be GPU accelerated simply by placing the initial condition array on the GPU. As a quick example:","category":"page"},{"location":"tutorials/within_method_gpu/","page":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","title":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","text":"using OrdinaryDiffEq, CUDA, LinearAlgebra\nfunction f(du, u, p, t)\n    mul!(du, A, u)\nend\n\nA = cu(-rand(3, 3))\nu0 = cu([1.0; 0.0; 0.0])\ntspan = (0.0f0, 100.0f0)\n\nprob = ODEProblem(f, u0, tspan)\nsol = solve(prob, Tsit5())\nsol = solve(prob, Rosenbrock23())","category":"page"},{"location":"tutorials/within_method_gpu/","page":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","title":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","text":"Notice that both stiff and non-stiff ODE solvers were used here.","category":"page"},{"location":"tutorials/within_method_gpu/","page":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","title":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","text":"note: Note\nTime span was changed to Float32 types, as GPUs generally have very slow Float64 operations, usually around 1/32 of the speed of Float32. cu(x) on an array automatically changes an Array{Float64} to a CuArray{Float32}. If this is not intended, use the CuArray constructor directly. For more information on GPU Float64 performance issues, search around Google for discussions like this.","category":"page"},{"location":"tutorials/within_method_gpu/","page":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","title":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","text":"warn: Warn\nFloat32 precision is sometimes not enough precision to accurately solve a stiff ODE. Make sure that the precision is necessary by investigating the condition number of the Jacobian. If this value is well-above 1e8, use Float32 with caution!","category":"page"},{"location":"tutorials/within_method_gpu/#Restrictions-of-CuArrays","page":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","title":"Restrictions of CuArrays","text":"","category":"section"},{"location":"tutorials/within_method_gpu/","page":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","title":"Within-Method GPU Parallelism of Ordinary Differential Equation Solves","text":"Note that all the rules of CUDA.jl apply when CuArrays are being used in the solver. While for most of the AbstractArray interface they act similarly to Arrays, such as having valid broadcasting operations (x .* y) defined, they will work on GPUs. For more information on the rules and restrictions of CuArrays, see this page from the CUDA.jl documentation.","category":"page"},{"location":"manual/choosing_ensembler/#Choosing-the-Ensemble:-EnsembleGPUArray-vs-EnsembleGPUKernel","page":"Choosing the Ensemble: EnsembleGPUArray vs EnsembleGPUKernel","title":"Choosing the Ensemble: EnsembleGPUArray vs EnsembleGPUKernel","text":"","category":"section"},{"location":"manual/choosing_ensembler/","page":"Choosing the Ensemble: EnsembleGPUArray vs EnsembleGPUKernel","title":"Choosing the Ensemble: EnsembleGPUArray vs EnsembleGPUKernel","text":"The short answer for how to choose an ensemble method is that, if EnsembleGPUKernel works on your problem, you should use it. A more complex discussion is the following:","category":"page"},{"location":"manual/choosing_ensembler/","page":"Choosing the Ensemble: EnsembleGPUArray vs EnsembleGPUKernel","title":"Choosing the Ensemble: EnsembleGPUArray vs EnsembleGPUKernel","text":"EnsembleGPUKernel is more asynchronous and has lower kernel call counts than EnsembleGPUArray. This should amount to lower overhead in any case where the algorithms are the same.\nEnsembleGPUKernel is restrictive on the types of ODE solvers that have been implemented for it. If the most efficient method is not in the list of GPU kernel methods, it may be more efficient to use EnsembleGPUArray with the better method.\nEnsembleGPUKernel requires equations to be written in out-of-place form, along with a few other restrictions, and thus in some cases can be less automatic than EnsembleGPUArray depending on how the code was originally written.\nCurrently, EnsembleGPUKernel does not support methods for stiff equations.","category":"page"},{"location":"manual/ensemblegpukernel/#EnsembleGPUKernel","page":"EnsembleGPUKernel","title":"EnsembleGPUKernel","text":"","category":"section"},{"location":"manual/ensemblegpukernel/#egk_doc","page":"EnsembleGPUKernel","title":"API","text":"","category":"section"},{"location":"manual/ensemblegpukernel/","page":"EnsembleGPUKernel","title":"EnsembleGPUKernel","text":"EnsembleGPUKernel","category":"page"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.EnsembleGPUKernel","page":"EnsembleGPUKernel","title":"DiffEqGPU.EnsembleGPUKernel","text":"EnsembleGPUKernel(backend,cpu_offload = 0.2)\n\nA massively-parallel ensemble algorithm which generates a unique GPU kernel for the entire ODE which is then executed. This leads to a very low overhead GPU code generation, but imparts some extra limitations on the use.\n\nPositional Arguments\n\nbackend: the KernelAbstractions backend for performing the computation.\ncpu_offload: the percentage of trajectories to offload to the CPU. Default is 0.2 or 20% of trajectories.\n\nLimitations\n\nNot all standard Julia f functions are allowed. Only Julia f functions which are capable of being compiled into a GPU kernel are allowed. This notably means that certain features of Julia can cause issues inside a kernel, like:\nAllocating memory (building arrays)\nLinear algebra (anything that calls BLAS)\nBroadcast\nOnly out-of-place f definitions are allowed. Coupled with the requirement of not allowing for memory allocations, this means that the ODE must be defined with StaticArray initial conditions.\nOnly specific ODE solvers are allowed. This includes:\nGPUTsit5\nGPUVern7\nGPUVern9\nTo use multiple GPUs over clusters, one must manually set up one process per GPU. See the multi-GPU tutorial for more details.\n\nExample\n\nusing DiffEqGPU, CUDA, OrdinaryDiffEq, StaticArrays\n\nfunction lorenz(u, p, t)\n    σ = p[1]\n    ρ = p[2]\n    β = p[3]\n    du1 = σ * (u[2] - u[1])\n    du2 = u[1] * (ρ - u[3]) - u[2]\n    du3 = u[1] * u[2] - β * u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nu0 = @SVector [1.0f0; 0.0f0; 0.0f0]\ntspan = (0.0f0, 10.0f0)\np = @SVector [10.0f0, 28.0f0, 8 / 3.0f0]\nprob = ODEProblem{false}(lorenz, u0, tspan, p)\nprob_func = (prob, i, repeat) -> remake(prob, p = (@SVector rand(Float32, 3)) .* p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func, safetycopy = false)\n\n@time sol = solve(monteprob, GPUTsit5(), EnsembleGPUKernel(), trajectories = 10_000,\n                  adaptive = false, dt = 0.1f0)\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#specialsolvers","page":"EnsembleGPUKernel","title":"Specialized Solvers","text":"","category":"section"},{"location":"manual/ensemblegpukernel/","page":"EnsembleGPUKernel","title":"EnsembleGPUKernel","text":"GPUTsit5\nGPUVern7\nGPUVern9\nGPUEM\nGPUSIEA\nGPURosenbrock23\nGPURodas4\nGPURodas5P\nGPUKvaerno3\nGPUKvaerno5","category":"page"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPUTsit5","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPUTsit5","text":"GPUTsit5()\n\nA specialized implementation of the 5th order Tsit5 method specifically for kernel generation with EnsembleGPUKernel. For a similar CPU implementation, see SimpleATsit5 from SimpleDiffEq.jl.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPUVern7","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPUVern7","text":"GPUVern7()\n\nA specialized implementation of the 7th order Vern7 method specifically for kernel generation with EnsembleGPUKernel.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPUVern9","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPUVern9","text":"GPUVern9()\n\nA specialized implementation of the 9th order Vern9 method specifically for kernel generation with EnsembleGPUKernel.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPUEM","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPUEM","text":"GPUEM()\n\nA specialized implementation of the Euler-Maruyama GPUEM method with weak order 1.0. Made specifically for kernel generation with EnsembleGPUKernel.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPUSIEA","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPUSIEA","text":"GPUSIEA()\n\nA specialized implementation of the weak order 2.0 for Ito SDEs GPUSIEA method specifically for kernel generation with EnsembleGPUKernel.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPURosenbrock23","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPURosenbrock23","text":"GPURosenbrock23()\n\nA specialized implementation of the W-method Rosenbrock23 method specifically for kernel generation with EnsembleGPUKernel.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPURodas4","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPURodas4","text":"GPURodas4()\n\nA specialized implementation of the Rodas4 method specifically for kernel generation with EnsembleGPUKernel.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPURodas5P","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPURodas5P","text":"GPURodas5P()\n\nA specialized implementation of the Rodas5P method specifically for kernel generation with EnsembleGPUKernel.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPUKvaerno3","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPUKvaerno3","text":"GPUKvaerno3()\n\nA specialized implementation of the Kvaerno3 method specifically for kernel generation with EnsembleGPUKernel.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.GPUKvaerno5","page":"EnsembleGPUKernel","title":"DiffEqGPU.GPUKvaerno5","text":"GPUKvaerno5()\n\nA specialized implementation of the Kvaerno5 method specifically for kernel generation with EnsembleGPUKernel.\n\n\n\n\n\n","category":"type"},{"location":"manual/ensemblegpukernel/#Lower-Level-API","page":"EnsembleGPUKernel","title":"Lower Level API","text":"","category":"section"},{"location":"manual/ensemblegpukernel/","page":"EnsembleGPUKernel","title":"EnsembleGPUKernel","text":"DiffEqGPU.vectorized_solve\nDiffEqGPU.vectorized_asolve\nDiffEqGPU.vectorized_map_solve","category":"page"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.vectorized_solve","page":"EnsembleGPUKernel","title":"DiffEqGPU.vectorized_solve","text":"vectorized_solve(probs, prob::Union{ODEProblem, SDEProblem}alg;\n                 dt, saveat = nothing,\n                 save_everystep = true,\n                 debug = false, callback = CallbackSet(nothing), tstops = nothing)\n\nA lower level interface to the kernel generation solvers of EnsembleGPUKernel with fixed time-stepping.\n\nArguments\n\nprobs: the GPU-setup problems generated by the ensemble.\nprob: the quintessential problem form. Can be just probs[1]\nalg: the kernel-based differential equation solver. Must be one of the EnsembleGPUKernel specialized methods.\n\nKeyword Arguments\n\nOnly a subset of the common solver arguments are supported.\n\n\n\n\n\n","category":"function"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.vectorized_asolve","page":"EnsembleGPUKernel","title":"DiffEqGPU.vectorized_asolve","text":"vectorized_asolve(probs, prob::ODEProblem, alg;\n                  dt = 0.1f0, saveat = nothing,\n                  save_everystep = false,\n                  abstol = 1.0f-6, reltol = 1.0f-3,\n                  callback = CallbackSet(nothing), tstops = nothing)\n\nA lower level interface to the kernel generation solvers of EnsembleGPUKernel with adaptive time-stepping.\n\nArguments\n\nprobs: the GPU-setup problems generated by the ensemble.\nprob: the quintessential problem form. Can be just probs[1]\nalg: the kernel-based differential equation solver. Must be one of the EnsembleGPUKernel specialized methods.\n\nKeyword Arguments\n\nOnly a subset of the common solver arguments are supported.\n\n\n\n\n\n","category":"function"},{"location":"manual/ensemblegpukernel/#DiffEqGPU.vectorized_map_solve","page":"EnsembleGPUKernel","title":"DiffEqGPU.vectorized_map_solve","text":"Lower level API for EnsembleArrayAlgorithm. Avoids conversion of solution to CPU arrays.\n\nvectorized_map_solve(probs, alg,\n                     ensemblealg::Union{EnsembleArrayAlgorithm}, I,\n                     adaptive)\n\nArguments\n\nprobs: the GPU-setup problems generated by the ensemble.\nalg: the kernel-based differential equation solver. Most of the solvers from OrdinaryDiffEq.jl        are supported.\nensemblealg: The EnsembleGPUArray() algorithm.\nI: The iterator argument. Can be set to for e.g. 1:10_000 to simulate 10,000 trajectories.\nadaptive: The Boolean argument for time-stepping. Use true to enable adaptive time-stepping.\n\nKeyword Arguments\n\nOnly a subset of the common solver arguments are supported.\n\n\n\n\n\n","category":"function"},{"location":"examples/bruss/#GPU-Acceleration-of-a-Stiff-Nonlinear-Partial-Differential-Equation","page":"GPU-Acceleration of a Stiff Nonlinear Partial Differential Equation","title":"GPU-Acceleration of a Stiff Nonlinear Partial Differential Equation","text":"","category":"section"},{"location":"examples/bruss/","page":"GPU-Acceleration of a Stiff Nonlinear Partial Differential Equation","title":"GPU-Acceleration of a Stiff Nonlinear Partial Differential Equation","text":"The following is a demonstration of a GPU-accelerated implicit solve of a stiff nonlinear partial differential equation (the Brusselator model):","category":"page"},{"location":"examples/bruss/","page":"GPU-Acceleration of a Stiff Nonlinear Partial Differential Equation","title":"GPU-Acceleration of a Stiff Nonlinear Partial Differential Equation","text":"using OrdinaryDiffEq, CUDA, LinearAlgebra\n\nconst N = 32\nconst xyd_brusselator = range(0, stop = 1, length = N)\nbrusselator_f(x, y, t) = (((x - 0.3)^2 + (y - 0.6)^2) <= 0.1^2) * (t >= 1.1) * 5.0\nlimit(a, N) = a == N + 1 ? 1 : a == 0 ? N : a\nkernel_u! = let N = N, xyd = xyd_brusselator, dx = step(xyd_brusselator)\n    @inline function (du, u, A, B, α, II, I, t)\n        i, j = Tuple(I)\n        x = xyd[I[1]]\n        y = xyd[I[2]]\n        ip1 = limit(i + 1, N)\n        im1 = limit(i - 1, N)\n        jp1 = limit(j + 1, N)\n        jm1 = limit(j - 1, N)\n        du[II[i, j, 1]] = α * (u[II[im1, j, 1]] + u[II[ip1, j, 1]] + u[II[i, jp1, 1]] +\n                           u[II[i, jm1, 1]] - 4u[II[i, j, 1]]) +\n                          B + u[II[i, j, 1]]^2 * u[II[i, j, 2]] - (A + 1) * u[II[i, j, 1]] +\n                          brusselator_f(x, y, t)\n    end\nend\nkernel_v! = let N = N, xyd = xyd_brusselator, dx = step(xyd_brusselator)\n    @inline function (du, u, A, B, α, II, I, t)\n        i, j = Tuple(I)\n        ip1 = limit(i + 1, N)\n        im1 = limit(i - 1, N)\n        jp1 = limit(j + 1, N)\n        jm1 = limit(j - 1, N)\n        du[II[i, j, 2]] = α * (u[II[im1, j, 2]] + u[II[ip1, j, 2]] + u[II[i, jp1, 2]] +\n                           u[II[i, jm1, 2]] - 4u[II[i, j, 2]]) +\n                          A * u[II[i, j, 1]] - u[II[i, j, 1]]^2 * u[II[i, j, 2]]\n    end\nend\nbrusselator_2d = let N = N, xyd = xyd_brusselator, dx = step(xyd_brusselator)\n    function (du, u, p, t)\n        @inbounds begin\n            ii1 = N^2\n            ii2 = ii1 + N^2\n            ii3 = ii2 + 2(N^2)\n            A = p[1]\n            B = p[2]\n            α = p[3] / dx^2\n            II = LinearIndices((N, N, 2))\n            kernel_u!.(Ref(du), Ref(u), A, B, α, Ref(II), CartesianIndices((N, N)), t)\n            kernel_v!.(Ref(du), Ref(u), A, B, α, Ref(II), CartesianIndices((N, N)), t)\n            return nothing\n        end\n    end\nend\np = (3.4, 1.0, 10.0, step(xyd_brusselator))\n\nfunction init_brusselator_2d(xyd)\n    N = length(xyd)\n    u = zeros(N, N, 2)\n    for I in CartesianIndices((N, N))\n        x = xyd[I[1]]\n        y = xyd[I[2]]\n        u[I, 1] = 22 * (y * (1 - y))^(3 / 2)\n        u[I, 2] = 27 * (x * (1 - x))^(3 / 2)\n    end\n    u\nend\nu0 = init_brusselator_2d(xyd_brusselator)\nprob_ode_brusselator_2d = ODEProblem(brusselator_2d, u0, (0.0, 11.5), p)\n\ndu = similar(u0)\nbrusselator_2d(du, u0, p, 0.0)\ndu[34] # 802.9807693762164\ndu[1058] # 985.3120721709204\ndu[2000] # -403.5817880634729\ndu[end] # 1431.1460373522068\ndu[521] # -323.1677459142322\n\ndu2 = similar(u0)\nbrusselator_2d(du2, u0, p, 1.3)\ndu2[34] # 802.9807693762164\ndu2[1058] # 985.3120721709204\ndu2[2000] # -403.5817880634729\ndu2[end] # 1431.1460373522068\ndu2[521] # -318.1677459142322\n\nprob_ode_brusselator_2d_cuda = ODEProblem(brusselator_2d, CuArray(u0), (0.0f0, 11.5f0), p,\n    tstops = [1.1f0])\nsolve(prob_ode_brusselator_2d_cuda, Rosenbrock23(), save_everystep = false);","category":"page"},{"location":"getting_started/#Getting-Started-with-GPU-Accelerated-Differential-Equations-in-Julia","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"","category":"section"},{"location":"getting_started/#The-two-ways-to-accelerate-ODE-solvers-with-GPUs","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"The two ways to accelerate ODE solvers with GPUs","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"There are two very different ways that one can accelerate an ODE solution with GPUs. There is one case where u is very big and f is very expensive but very structured, and you use GPUs to accelerate the computation of said f. The other use case is where u is very small, but you want to solve the ODE f over many different initial conditions (u0) or parameters p. In that case, you can use GPUs to parallelize over different parameters and initial conditions. In other words:","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"Type of Problem SciML Solution\nAccelerate a big ODE Use CUDA.jl's CuArray as u0\nSolve the same ODE with many u0 and p Use DiffEqGPU.jl's EnsembleGPUArray and EnsembleGPUKernel","category":"page"},{"location":"getting_started/#Supported-GPUs","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Supported GPUs","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"SciML's GPU support extends to a wide array of hardware, including:","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"GPU Manufacturer GPU Kernel Language Julia Support Package Backend Type\nNVIDIA CUDA CUDA.jl CUDA.CUDABackend()\nAMD ROCm AMDGPU.jl AMDGPU.ROCBackend()\nIntel OneAPI OneAPI.jl oneAPI.oneAPIBackend()\nApple (M-Series) Metal Metal.jl Metal.MetalBackend()","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"For this tutorial we will demonstrate the CUDA backend for NVIDIA GPUs, though any of the other GPUs can be used by simply swapping out the backend choice.","category":"page"},{"location":"getting_started/#Simple-Example-of-Within-Method-GPU-Parallelism","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Simple Example of Within-Method GPU Parallelism","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"The following is a quick and dirty example of doing within-method GPU parallelism. Let's say we had a simple but large ODE with many linear algebra or map/broadcast operations:","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"using OrdinaryDiffEq, LinearAlgebra\nu0 = rand(1000)\nA = randn(1000, 1000)\nf(du, u, p, t) = mul!(du, A, u)\nprob = ODEProblem(f, u0, (0.0, 1.0))\nsol = solve(prob, Tsit5())","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"Translating this to a GPU-based solve of the ODE simply requires moving the arrays for the initial condition, parameters, and caches to the GPU. This looks like:","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"using OrdinaryDiffEq, CUDA, LinearAlgebra\nu0 = cu(rand(1000))\nA = cu(randn(1000, 1000))\nf(du, u, p, t) = mul!(du, A, u)\nprob = ODEProblem(f, u0, (0.0f0, 1.0f0)) # Float32 is better on GPUs!\nsol = solve(prob, Tsit5())","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"Notice that the solution values sol[i] are CUDA-based arrays, which can be moved back to the CPU using Array(sol[i]).","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"More details on effective use of within-method GPU parallelism can be found in the within-method GPU parallelism tutorial.","category":"page"},{"location":"getting_started/#Example-of-Parameter-Parallelism-with-GPU-Ensemble-Methods","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Example of Parameter-Parallelism with GPU Ensemble Methods","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"On the other side of the spectrum, what if we want to solve tons of small ODEs? For this use case, we would use the ensemble methods to solve the same ODE many times with different parameters. This looks like:","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"using DiffEqGPU, OrdinaryDiffEq, StaticArrays, CUDA\n\nfunction lorenz(u, p, t)\n    σ = p[1]\n    ρ = p[2]\n    β = p[3]\n    du1 = σ * (u[2] - u[1])\n    du2 = u[1] * (ρ - u[3]) - u[2]\n    du3 = u[1] * u[2] - β * u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nu0 = @SVector [1.0f0; 0.0f0; 0.0f0]\ntspan = (0.0f0, 10.0f0)\np = @SVector [10.0f0, 28.0f0, 8 / 3.0f0]\nprob = ODEProblem{false}(lorenz, u0, tspan, p)\nprob_func = (prob, i, repeat) -> remake(prob, p = (@SVector rand(Float32, 3)) .* p)\nmonteprob = EnsembleProblem(prob, prob_func = prob_func, safetycopy = false)\n\nsol = solve(monteprob, GPUTsit5(), EnsembleGPUKernel(CUDA.CUDABackend()),\n    trajectories = 10_000)","category":"page"},{"location":"getting_started/","page":"Getting Started with GPU-Accelerated Differential Equations in Julia","title":"Getting Started with GPU-Accelerated Differential Equations in Julia","text":"To dig more into this example, see the ensemble GPU solving tutorial.","category":"page"},{"location":"#DiffEqGPU:-Massively-Data-Parallel-GPU-Solving-of-ODEs","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"","category":"section"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"This library is a component package of the DifferentialEquations.jl ecosystem. It includes functionality for making use of GPUs in the differential equation solvers.","category":"page"},{"location":"#Installation","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"Installation","text":"","category":"section"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"To install DiffEqGPU.jl, use the Julia package manager:","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"using Pkg\nPkg.add(\"DiffEqGPU\")","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"This will also install all the dependencies, including the CUDA.jl, which will also install all the required versions of CUDA and CuDNN required by these libraries. Note that the same requirements of CUDA.jl apply to DiffEqGPU, such as requiring a GPU with CUDA v11 compatibility. For more information on these requirements, see the requirements of CUDA.jl.","category":"page"},{"location":"#Contributing","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"Contributing","text":"","category":"section"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"#Reproducibility","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"</details>","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"</details>","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"</details>","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"You can also download the\n<a href=\"","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n       \"/assets/Manifest.toml\"","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n       \"/assets/Project.toml\"","category":"page"},{"location":"","page":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","title":"DiffEqGPU: Massively Data-Parallel GPU Solving of ODEs","text":"\">project</a> file.","category":"page"},{"location":"examples/reaction_diffusion/#GPU-Accelerated-Stochastic-Partial-Differential-Equations","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"GPU-Accelerated Stochastic Partial Differential Equations","text":"","category":"section"},{"location":"examples/reaction_diffusion/","page":"GPU-Accelerated Stochastic Partial Differential Equations","title":"GPU-Accelerated Stochastic Partial Differential Equations","text":"<meta http-equiv=\"Refresh\" content=\"0; url='https://docs.sciml.ai/Overview/stable/showcase/gpu_spde/'\" />","category":"page"}]
}
